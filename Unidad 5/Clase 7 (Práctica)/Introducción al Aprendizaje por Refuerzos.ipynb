{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Aprendizaje por Refuerzos\n",
    "\n",
    "* Introducción\n",
    "* Librería OpenAI Gym\n",
    "* Evaluación del rendimiento en un algoritmo de RL\n",
    "* Híper-parámetros en RL\n",
    "\n",
    "5to año - Ingeniería en Sistemas de Información\n",
    "\n",
    "Facultad Regional Villa María"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "RL, informalmente y a fines prácticos, es una forma de aprendizaje que guía la búsqueda (con parte estocástica) de las acciones para que las mismas tiendan a converger en aquellas que maximicen la recompensa recibida.\n",
    "\n",
    "En la presente clase vamos a introducir algunas de las herramientas disponibles para trabajar con aprendizaje por refuerzos (RL). A nivel de implementación, a diferencia de lo que sucede con el aprendizaje supervisado y no supervisado, RL resulta más complejo porque su implementación depende mucho del dominio del problema.\n",
    "\n",
    "Esto se debe a que en RL no realizamos el aprendizaje a partir de datasets fijos sino por medio de la interacción con el entorno, el cual puede ser real o simulado. El \"dataset\" en RL lo obtenemos a partir de las trazas de ejecución\n",
    "\n",
    "$$T_0 = (s_{00}, a_{00}, r_{01}, s_{01}, a_{01}, \\dots, r_{0t}, s_{0t})$$\n",
    "$$T_1 = (s_{10}, a_{10}, r_{11}, s_{11}, a_{11}, \\dots, r_{1r}, s_{1r})$$\n",
    "$$\\dots$$\n",
    "$$T_N = (\\dots)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un problema de RL dado, el entorno puede ser real (en donde las acciones impactan directamente en un sistema real) o bien simulado (en donde el entorno está bajo el control de quién simula).\n",
    "\n",
    "Para entornos simulados, existen una serie de librerías tanto de entornos como de agentes que nos permiten empezar a entrenar nuestros algoritmos de arranque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Librería OpenAI Gym\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) (Brockman et. al., 2016) es una librería de OpenAI que ofrece entornos y una interfaz estándar con la cuál probar nuestros agentes. Su objetivo es proveer benchmarks unificados para ver el desempeño de algoritmos en el entorno y así poder saber con facilidad cómo es su desempeño comparado con los demás. Parte de la siguiente sección está basada en la documentación oficial de OpenAI.\n",
    "\n",
    "Vamos a instalar el gym..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a instalar el gym\n",
    "\n",
    "try:\n",
    "    from pip import main as pipmain\n",
    "except:\n",
    "    from pip._internal import main as pipmain\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "pipmain(['install', 'gym'])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interfaz principal de los ambientes de gym es la interfaz Env. La misma posee tres métodos principales (info. basada en la documentación oficial de Gym):\n",
    "\n",
    "\n",
    "    reset(self): Reinicia el estado del entorno, a su estado inicial, devolviendo una observación de dicho estado.\n",
    "    step(self, action): \"Avanza\" un timestep del ambiente. Devuelve: observation, reward, done, info.\n",
    "    render(self): Muestra en pantalla una parte del ambiente.\n",
    "    close(self): Finaliza con la instancia del agente.\n",
    "    seed(self): Establece la semilla aleatoria del generador de números aleatorios del presente entorno.\n",
    "\n",
    "\n",
    "Por otra parte, cada entorno posee los siguientes tres atributos principales:\n",
    "\n",
    "    action_space: El objeto de tipo Space correspondiente al espacio de acciones válidas.\n",
    "    observation_space: El objeto de tipo Space correspondiente a todos los rangos posibles de observaciones.\n",
    "    reward_range: Tupla que contiene los valores mínimo y máximo de recompensa posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: antes de poder ver video instalar la dependencia ffmpeg. En Linux esto se hace mediante el comando\n",
    "\n",
    "    sudo apt-get install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # se ejecuta una acción aleatoria\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "env = gym.make('MountainCar-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del rendimiento en un algoritmo de RL\n",
    "\n",
    "A diferencia de lo que sucede en el aprendizaje supervisado, en el aprendizaje por refuerzos el rendimiento se evalúa por una función específica que es la función de recompensa. En la práctica, la función de recompensa puede ser externa (y proveer desde el entorno) o bien puede ser una función creada por diseño (a modo de dirigir el agente hacia lo que por diseño se considera mejor) o bien combinar ambos enfoques (usando recompensas obtenidas desde el entorno y generadas por diseño).\n",
    "\n",
    "Como el objetivo de RL es maximizar la recompensa obtenida, es posible utilizar la información sobre la obtención de la recompensas en cada time-step o episodio para evaluar el rendimiento parcial del agente (esto depende mucho de la particularidad de la distribución de la recompensa para el problema tratado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo:\n",
    "\n",
    "![Example](images/rl_convergence_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Híper-parámetros de RL\n",
    "\n",
    "Los algoritmos de RL, al igual que como sucede con las demás técnicas de ML, poseen híper-parámetros $\\theta$ (no confundir con $\\theta$ usado para denotar el umbral de decisión en la clasificación) que regulan la forma en la que se realiza el aprendizaje. En RL en particular, de $\\theta$ **depende la generación de los datos**, y los mismos sesgan de tal forma la ejecución que la mayoría de las veces no es posible comparar las distintas trazas generadas con distintos $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizar los híper-parámetros $\\theta$ en algoritmos de RL es un desafío considerable, debido a que de los mismos **depende la generación de los datos**, y los híper-parámetros sesgan de tal forma la ejecución que la mayoría de las veces no es posible comparar las distintas trazas generadas con distintos $\\theta$.\n",
    "\n",
    "En contrapartida, en el aprendizaje supervisado y no supervisado, al aplicar un algoritmo de ML los datos ya están generados de antemano, siendo el algoritmo (informalmente) una forma de obtener conocimiento a partir de los datos crudos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si además también consideramos que en RL, la generación de una traza es algo computacionalmente costoso (excepto en ejemplos pequeños como el mostrado en esta clase), la selección de $\\theta$ debe hacerse muy cuidadosamente. En la práctica suele hacerse manualmente o bien alguna variante de random search (grid search cuando el ejemplo es computacionalmente demandante es inviable).\n",
    "\n",
    "El gran problema de ambos métodos es que actualmente no existe un esquema formal para aprovechar el conocimiento adquirido de los $\\theta$ descartados, quedando la optimización sujeta a prueba y error o reglas empíricas. En RL particularmente esto no es un dato menor, porque al ser tan sensibles los modelos a sus híper-parámetros, una nueva simulación con un ligero cambio de éstos puede generar resultados totalmente distintos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ejemplo de algoritmo propuesto para optimización de híper-parámetros en RL (por los docentes de la cátedra)](https://arxiv.org/pdf/1805.04748)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo práctico 6\n",
    "\n",
    "Para el presente trabajo práctico vamos a utilizar las clases CartPoleTabularAgent y QLearning, llamadas desde el presente notebook, y el script *cartpole_main_script*. Las mismas se presentan como herramientas para resolver los ejercicios, por lo se permite, para la resolución de los mismos, modificarlas o reemplazarlas por sus propias implementaciones. Las clases presentan la siguiente funcionalidad:\n",
    "\n",
    "> cartpole_main_script\n",
    "\n",
    "Script que crea y define la configuración inicial del agente de RL de tipo CartPole, creando la instancia de CartPoleTabularAgent.\n",
    "\n",
    "> CartPoleTabularAgent\n",
    "\n",
    "Clase que implementa la interfaz con OpenAIGym, creando el entorno e iterando el mismo. Adicionalmente provee una interfaz para llamar y correr el agente RL. La clase adicionalmente le realiza un pre-procesamiento a las observaciones del entorno: puesto que las mismas son continuas, el espacio de estados es discretizado con respecto a atributos tales como la posición en la que se encuentra el carro y su ángulo.\n",
    "\n",
    "> QLearning\n",
    "\n",
    "Clase que implementa el algoritmo QLearning, lo cual involucra el guardado de los valores de Q en un diccionario, la selección de acciones (mediante $choose\\_action$) y la actualización de los valores de Q (mediante $learn$).\n",
    "\n",
    "\n",
    "### Ejercicios Básicos\n",
    "\n",
    "1. Graficar una curva de convergencia para mostrar cómo el algoritmo va realizando sucesivamente su aprendizaje (por ejemplo mostrando la recompensa promedio por episodio). Si la convergencia en la curva no es clara (por ejemplo porque salta constantemente en el rango de valores), debe de alguna forma suavisarse (por ejemplo al dividirla en 10 segmentos y promediar el valor de recompensa en los mismos).\n",
    "2. Implementar un algoritmo que cambie la política de $\\epsilon$-greedy a Softmax, es decir que para todo estado $s$ y toda acción $a$, la probabilidad de elegir la acción $a$ debe estar dada por\n",
    "$$\\pi(a \\mid s) = \\frac{e^{Q(s,a)/\\tau}}{\\sum_{a'}e^{Q(s,a')/\\tau}}$$\n",
    "3. Probar con distintas configuraciones de híper-parámetros (ejemplo: gamma, la cantidad de divisiones de la posición del carro, entre otros) y comentar cómo las mismas cambian la convergencia del agente (tal prueba puede también realizarse mediante una optimización de híper-parámetros estilo Grid Search o Random Search).\n",
    "\n",
    "### Ejercicios Complementarios\n",
    "\n",
    "1. Implementar algoritmo $SARSA$.\n",
    "2. Implementar $Q(\\lambda)$ o $SARSA(\\lambda)$.\n",
    "3. Comparar los algoritmos implementados en cuanto a la curva de convergencia planteada en el Ej. Básico 1.\n",
    "\n",
    "### Ejercicios Extra\n",
    "\n",
    "1. En el Ejercicio Complementario 2, implementar tanto $Q(\\lambda)$ como $SARSA(\\lambda)$ e incluir ambos en la comparación gráfica.\n",
    "\n",
    "### Links adicionales para aquellos interesados\n",
    "\n",
    "* [Próximamente: OpenAI libera detalles de su algoritmo de RL para Dota 2 con el que enfrentará a profesionales en The International](https://blog.openai.com/openai-five/)\n",
    "\t- [Actualización: primera serie de partidos pre-The International finalizada](https://blog.openai.com/openai-five-benchmark-results/)\n",
    "* [Curso de Deep Reinforcement Learning que incluye juegos Doom y Mario Bros](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "* [Paper Human-Level Control through Deep Reinforcement Learning (RL para juegos de Atari)](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf) y su [implementación en Python](https://github.com/devsisters/DQN-tensorflow)\n",
    "* [Tutorial para entrenar agentes de DeepRL en StarCraft II](http://chris-chris.ai/2017/08/30/pysc2-tutorial1/)\n",
    "* [Buena librería de agentes de RL](http://ray.readthedocs.io/en/latest/rllib.html)\n",
    "\n",
    "\n",
    "\n",
    "Fecha de entrega: **6/7/2018 23:55**\n",
    "\n",
    "Nota: la resolución de los ejercicios es **individual**. La reutilización del código de los notebooks está permitida (por ejemplo para confeccionar gráficos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recomendación**: No se sugiere hacer este TP desde jupyter notebook sino desde un IDE estilo Pycharm, debido a que los algoritmos de RL suelen requerir un debug paso a paso, tanto para corregir errores como para entender mejor cómo funcionan los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código de llamado a la interfaz de CartPoleTabularAgent (el mismo código se encuentra en el script *cartpole_main_script*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cart_pole_tabular_agent.CartPoleTabularAgent as cP\n",
    "\n",
    "# se declara una semilla aleatoria\n",
    "random_state = np.random.RandomState(20)\n",
    "\n",
    "# el tiempo de corte del agente son 200 time-steps (el cual es el máximo del entorno Cartpole; seguir iterando tras 200\n",
    "# no cambiará el entorno)\n",
    "cutoff_time = 200\n",
    "\n",
    "# instanciamos nuestro agente\n",
    "agent = cP.CartPoleTabularAgent()\n",
    "\n",
    "# definimos sus híper-parámetros básicos\n",
    "# (también podrían establecerse los bins que hacen la división, modificando el método set_hyper_parameters)\n",
    "\n",
    "agent.set_hyper_parameters({\"alpha\": 0.5, \"gamma\": 0.9, \"epsilon\": 0.1})\n",
    "\n",
    "# declaramos como True la variable de mostrar video, para ver en tiempo real cómo aprende el agente. Borrar esta línea\n",
    "# para acelerar la velocidad del aprendizaje\n",
    "agent.display_video = True\n",
    "\n",
    "# establece el tiempo de\n",
    "agent.set_cutoff_time(cutoff_time)\n",
    "\n",
    "# inicializa el agente\n",
    "agent.init_agent()\n",
    "\n",
    "# reinicializa el conocimiento del agente\n",
    "agent.restart_agent_learning()\n",
    "\n",
    "# run corre el agente devuelve el overall score, que es el promedio de recompensa de todos los episodios\n",
    "overall_score = agent.run()\n",
    "agent.destroy_agent()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
