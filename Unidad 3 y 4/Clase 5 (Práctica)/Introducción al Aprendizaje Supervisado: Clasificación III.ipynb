{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Aprendizaje Supervisado - Clasificación (Parte III)\n",
    "\n",
    "* Vista general de algunos métodos de clasificación.\n",
    "* Enfoques prácticos para problemas comunes en AS.\n",
    "* Ejemplo de sub-área clásica de IA: Natural Language Processing (NLP)\n",
    "* Conclusiones del aprendizaje supervisado.\n",
    "\n",
    "5to año - Ingeniería en Sistemas de Información\n",
    "\n",
    "Facultad Regional Villa María"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vista general de algunos métodos de clasificación\n",
    "\n",
    "La presente sección se deja solamente a modo de repaso general de los contenidos ya vistos en el teórico. Para su estudio en profundidad, referirse a las clases teóricas en donde fueron presentados los temas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes Classifier\n",
    "\n",
    "Retomando el Teorema de Bayes, dada una observación (una fila de $X$) que asumimos **IID** $x_1, x_2, ..., x_n$, \n",
    "\n",
    "$$P(y \\mid x_1, x_2, ..., x_n) = \\frac{P(y) P(x_1, x_2, ..., x_n \\mid y)}{P(x_1, x_2, ..., x_n)}$$\n",
    "\n",
    "... Naïve Bayes Classifier es un clasificador que intenta aproximar $P(y \\mid x_1, x_2, ..., x_n)$ tomando la muy simplista (_naïve_) asunción de que los predictores son independientes entre sí. Analíticamente,\n",
    "\n",
    "$$P(x_i \\mid y, x_1, ..., x_{i-1}, x_{i+1}, ..., x_n) = P(x_i \\mid y)$$\n",
    "\n",
    "Recordemos que dados $A$, $B$, si $A$ es independiente de $B$ entonces $P(A \\cap B) = P(A)P(B)$. Entonces la primera ecuación se puede reescribir como\n",
    "\n",
    "$$P(y \\mid x_1, x_2, ..., x_n) = \\frac{P(y) \\prod_{i=1}^n P(x_i \\mid y)}{P(x_1, x_2, ..., x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando que $P(x_1, x_2, ..., x_n) = \\sum_{c=1}^C P(y_c)P(x_1, x_2, ..., x_n \\mid y_c)$ es constante, se deduce que\n",
    "\n",
    "$$P(y \\mid x_1, x_2, ..., x_n) \\varpropto P(y) \\prod_{i=1}^n P(x_i \\mid y)$$\n",
    "\n",
    "Por lo tanto,\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^n P(x_i \\mid y)$$\n",
    "\n",
    "* Pese a la simplista asunción tomada, Naïve Bayes es uno de los mejores y más rápidos clasificadores, especialmente en lo referido a procesamiento de texto. \n",
    "* Por otra parte, no es considerado un buen estimador, por lo que las probabilidades estimadas (por medio de *predict_proba*) no deben tomarse muy seriamente. Tampoco resulta ideal para datasets con muchos features numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Una pregunta común es ¿por qué no utilizar los métodos de regresión para predecir las clases?\n",
    "\n",
    "* La dificultad es que la mayoría de los métodos de regresión trabajan asumiendo un orden sobre las salidas. Como las salidas no están ordenadas, no es posible utilizar la diferencia entre ellas para entrenar iterativamente el modelo.\n",
    "\n",
    "* Supongamos que queremos predecir la condición clínica de un paciente que llega a emergencias en base a sus síntomas. Su condición clínica puede ser {Aflicción cardíaca, Sobredosis, Reacción alérgica}. Podríamos intentar hacer una regresión de las salidas como variables cuantitativas de la siguiente forma\n",
    "\n",
    "0 - Aflicción cardíaca\n",
    "\n",
    "1 - Sobredosis\n",
    "\n",
    "2 - Reacción alérgica\n",
    "\n",
    "* Un problema es que estamos asumiendo un orden de las salidas, en donde asumimos sin fundamentos 1) que una SD está entre una AC y una RA y 2) que la diferencia entre una AC y una SD es la misma que la diferencia entre SD y RA.\n",
    "\n",
    "* Este problema podría solventarse si limitamos las clases a dos. En ese sentido, supongamos que limitamos la cantidad de clases a AC y SD; una predicción de 0.23 se asociaría con una AC, mientras que una predicción de 0.9 se asociaría con una SD. Aquí el otro problema: supongamos que usamos un método de regresión lineal, ¡el valor de predicción iría hacia el infinito, pudiendo incluir probabilidades negativas o > 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No obstante, es posible solventar esto aplicándole un \"aplanamiento\" a las salidas para que pasen de $[-\\infty,\\infty]$ a probabilidades [0,1].\n",
    "* Uno de los métodos clásicos que realizan esto recibe el nombre de regresión logística (**logistic regression**). El mismo propone modelar $P(y \\mid X)$ directamente (es un método discriminativo) a través de la función logística. Para el caso de dos clases 0 y 1:\n",
    "$$P(y = 1 \\mid X = X_0) = \\frac{\\exp(\\beta _{0} + \\beta^{T} X_0)}{1+\\exp(\\beta _{0}+\\beta ^{T} X_0)}$$\n",
    "donde $\\beta$ son los coeficientes de regresión.\n",
    "(Notar que $\\exp(x)$ equivale a $e^x$).\n",
    "\n",
    "![Logistic Regression](images/logistic_regression.png)\n",
    "\n",
    "Fuente: Figura 4.2 de Hastie et. al. 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los coeficientes $\\beta$ son desconocidos. Para estimarlos se utiliza el método de _Maximum Likelihood_. Recordemos que en estadística, la likelihood (verosimilitud) de un modelo es la probabilidad de los parámetros del mismo dados los datos. Así, la función de likelihood está dada por $\\mathbb{L}(\\theta \\mid X) = P(x_1, x_2, ..., x_n \\mid \\theta) = P(x_1 \\mid \\theta) \\times P(x_2 \\mid \\theta) \\times ... \\times P(x_n \\mid \\theta) = \\prod_{i=1}^n P(x_i \\mid \\theta)$.\n",
    "\n",
    "* El objetivo del método Maximum Likelihood es encontrar los parámetros $\\theta$ para los cuales se maximiza la verosimilitud de los datos (en otras palabras, bajo qué parámetros es más probable que los datos hayan sido generados). Este método también puede utilizarse para verificar si dadas unas muestras que por ejemplo se asumen bajo una distribución normal, cuáles son los parámetros $\\mu$ y $\\sigma$ de la misma.\n",
    "\n",
    "* Dados los datos IID $x_1, x_2, ..., x_n$, el máximo likelihood está dado por\n",
    "$$\\theta_{ML} = \\arg \\max_{\\theta \\in \\Theta} \\hat{\\mathbb{l}}(\\theta \\mid x_1, x_2,..., x_n)$$\n",
    "\n",
    "* Logistic Regression es uno de los clásicos para problemas de clasificación binaria. Para clasificaciones multiclase, se utiliza el esquema One vs Rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "* Una SVM puede ser imaginada como una superficie que define un límite entre varios puntos de datos, los cuales representan ejemplos de distintas clases.\n",
    "* El objetivo de una SVM es crear un límite, llamado hiperplano, que separe las particiones de datos de la forma más homogénea y con mayor distancia posible.\n",
    "* Es uno de los mejores métodos de clasificación, habiendo explotado en popularidad en los últimos años. Puede ser adaptado para casi cualquier problema de aprendizaje; su enorme flexibilidad hace que sea un excelente método empleado en campos como reconocimiento de patrones en una imágenes, procesamiento de texto y detección de eventos muy raros.\n",
    "\n",
    "![SVM](images/hyper-planes.png)\n",
    "\n",
    "Fuente: https://pradeepadhokshaja.blogspot.com.ar/2016/06/optical-character-recognition-using.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En un espacio $d$-dimensional, un hiperplano se define como un sub-espacio de dimensión ($d-1$). Ejemplo: en tres dimensiones, un hiperplano equivale al plano tal como fue estudiado en Geometría Analítica. Recordemos que un plano es el análogo en dos dimensiones a un punto ($d=0$) y a una línea ($d=1$).\n",
    "\n",
    "* En dos dimensiones, un hiperplano se define por la ecuación \n",
    "$$\\beta_0 + \\beta_1 x + \\beta_2 y = 0$$\n",
    "Dado un punto $(x^*,y^*)$ que no satisface la anterior ecuación sino que\n",
    "$$\\beta_0 + \\beta_1 x^* + \\beta_2 y^* > 0$$\n",
    "decimos que $(x^*,y^*)$ se encuentra hacia uno de los lados del plano.\n",
    "Análogamente, si $$\\beta_0 + \\beta_1 x^* + \\beta_2 y^* < 0$$ diremos que $(x^*,y^*)$ se encuentra hacia el otro lado del plano (si esto no queda claro a simple vista, imaginar el ejemplo de una recta. Se muestran ejemplos en la figura).\n",
    "\n",
    "![](images/hyperplanes.png)\n",
    "\n",
    "Fuente: https://pradeepadhokshaja.blogspot.com.ar/2016/06/optical-character-recognition-using.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extendiendo esta noción a un espacio $d$-dimensional, y considerando que queremos clasificar cada observación en el conjunto binario de clases $C = \\{-1,1\\}$, de forma que \n",
    "$$\\beta_0 + \\beta_1 x_1^* +...+ \\beta_d x_d^* > 0$$\n",
    "si la observación $y_i=1$, y\n",
    "$$\\beta_0 + \\beta_1 x_1^* +...+ \\beta_d x_d^* < 0$$ si la observación $y_i = -1$\n",
    "\n",
    "* Suponiendo que existe un hiperplano capaz de separar perfectamente las observaciones de acuerdo a cada clase, el mismo consituye un clasificador que naturalmente separa las clases clasificadas como -1 de las clasificadas con 1, de forma que\n",
    "$$y_i (\\beta_0 + \\beta_1 X_{i1}^* +...+ \\beta_d X_{id}^*) > 0$$\n",
    "dada una matriz X de datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esencialmente, lo que hace una SVM es encontrar aquel hiperplano que maximiza el margen que separa las clases (**maximum margin hyperplane, MMH**), puesto la misma es la que se espera minimice el error de clasificación. Aquellos puntos ubicados en tales márgenes se denominan **support vectors**.\n",
    "\n",
    "![](images/maximum_margin.png)\n",
    "\n",
    "Fuente: https://pradeepadhokshaja.blogspot.com.ar/2016/06/optical-character-recognition-using.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Un problema se origina cuando los datos con los que contamos no son linealmente separables como en el caso de la figura anterior.\n",
    "* En tal caso, dos enfoques suelen usarse. En el primero, el algoritmo utiliza emplea un margen flexible denotado por $\\xi$, además de una función de costo $C$ que es aplicada para los casos en los cuales un punto no pertenece correctamente a su hiperplano. El objetivo de SVM pasa ahora a ser la minimización de la función de costo $C$.\n",
    "\n",
    "![](images/soft_margin.png)\n",
    "\n",
    "Fuente: https://pradeepadhokshaja.blogspot.com.ar/2016/06/optical-character-recognition-using.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El segundo enfoque, por otra parte, consiste en utilizar un operador (kernel) no lineal en lugar de utilizar el producto punto, de modo de poder generar un MMH en otro espacio.\n",
    "\n",
    "![](images/kernel_trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "* Un árbol de decisión es un modelo predictivo que intenta explicar los datos $(X, y)$ como conjuntos de **reglas de decisión**. Recibe ese nombre porque puede representarse como una estructura de árbol.\n",
    "* Se compone de dos tipos de nodos: los internos y las hojas. Los nodos internos definen reglas de decisión que consultan si una determinada condición es satisfecha. Cada nodo no hoja amplía en 1 la profundidad del árbol.\n",
    "* Las hojas, por su parte, tienen un valor de predicción (para el caso de los árboles de regresión) o una clase (para los árboles de clasificación), dependiendo del problema que estén resolviendo.\n",
    "* Técnica con altos rendimientos e interpretable, pero **muy propensa al overfitting**: la profundidad elegida del árbol es muy importante; es deseable \"podar\" el árbol al reducir su profundidad para evitar que el overfitting llegue a límites muy altos.\n",
    "\n",
    "![Regla de decisión](images/decision_rule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Árbol de decisión](images/obama_tree.png)\n",
    "\n",
    "Fuente: New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "* Los métodos de Ensemble Learning combinan varios modelos para resolver un problema de predicción. Cada uno de esos modelos aprenden y realizan predicciones independientemente, para luego combinar las mismas de forma tal de generar una única predicción igual o mejor que cualquier predicción realizada por un único modelo.\n",
    "* Se conocen como modelos débiles, puesto que necesitan de modelos específicos para poder combinar sus predicciones.\n",
    "* Un método muy conocido de ensemble es **Bootstrap Aggregating** (conocido como **bagging**).\n",
    "\n",
    "![](images/bagging.png)\n",
    "\n",
    "Fuente: Udacity Course - Machine Learning for Trading (por Georgia Tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "* Random Forest es un método de ensemble learning que agrega múltiples árboles de decisión.\n",
    "* Es una mejora del bagging, que es un método que hace bootstrapping sobre varios modelos y promedia sus salidas para obtener la media (para problemas de regresión) o la clase seleccionada por votación (para problemas de clasificación).\n",
    "* Por medio de bootstrapping sobre cada una de las divisiones del árbol, los Random Forest crean automáticamente una alta cantidad de árboles de decisión sobre los mismos, con el objetivo de encontrar árboles de decisión que  obteniendo la salida tomando la media de todas las predicciones de los árboles creados para problemas de regresión, o bien eligiendo la clase de salida mediante votación para problemas de clasificación.\n",
    "* La gran mayoría de tales árboles generados automáticamente arrojan pésimas predicciones; no obstante las mismas se cancelan entre sí dando lugar a aquellos árboles que mejor se ajustan a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ventajas\n",
    "* Tiene rendimientos a nivel del estado del arte, al igual que las SVM.\n",
    "\n",
    "Desventajas\n",
    "* Es un método muy propenso al overfitting.\n",
    "* Al usar los árboles de decisión de esta manera, se pierde bastante de su interpretabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoques prácticos para problemas comunes en Aprendizaje Supervisado\n",
    "\n",
    "#### Outliers\n",
    "\n",
    "* Un _outlier_ es un punto que presenta una anomalía con respecto a nuestros demás datos. En la regresión, se considera también como outliers a aquellos puntos muy específicos para los cuales nuestra predicción $\\hat{y}_i$ se encuentra muy lejos del valor real $y_i$.\n",
    "\n",
    "![](images/outlier.jpg)\n",
    "\n",
    "Fuente: https://statsland.wordpress.com/2012/09/24/outliers-are-they-good-or-bad/\n",
    "\n",
    "* Tales puntos suelen ser particularmente molestos, ya que no podemos explicar por qué la predicción está tan lejos, y al ser muy baja su cantidad no afectan demasiado el error global, por lo que cambiar el modelo sólo por ellos no tiene mucho sentido.\n",
    "\n",
    "* El enfoque más común (pero muchas veces incorrecto) es asumir que fueron producto de un error en la toma de datos y eliminarlo.\n",
    "\n",
    "* De alguna forma los outliers tienen que ser considerados (como mínimo tener el registro de que ocurrieron); pues a menudo suelen significar que existe un feature que no fue considerado en la toma o generación de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción multi-label\n",
    "\n",
    "* Para algunos datasets, las observaciones están etiquetadas con más de una salida, las cuáles no son excluyentes entre sí.\n",
    "\n",
    "* Un enfoque común para estos casos es utilizar un predictor (regresor o clasificador) por cada uno de los labels. En el caso de la clasificación, este enfoque consiste en utilizar un clasificador binario OneVsRest para cada una de las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset con información faltante\n",
    "\n",
    "* Existen casos donde los datasets no contienen información para todos sus features. Para estos casos suelen tomarse dos enfoques. Uno es eliminar las observaciones afectadas del dataset. \n",
    "\n",
    "* Otro enfoque consiste en utilizar un predictor (ej Random Forest) para estimar, en base a las demás observaciones que contienen valor en el feature, cuál es el valor que podría tener ese feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of Dimensionality\n",
    "\n",
    "* _The Curse of Dimensionality_ (Bellman, 1957) se refiere al problema donde, a medida que crece linealmente la cantidad de dimensiones de nuestros datos, la complejidad inherente de procesarlas crece a la vez en un orden exponencial.\n",
    "\n",
    "* En ML, esto tiene dos consecuencias principales. La primera es que a medida que aumentan los features (es decir la *dimensión* $d$ del dataset, se necesitan cada vez más datos para tener una muestra representativa de los mismos que abarque una parte significativa de las combinaciones de todos los features.\n",
    "\n",
    "* La segunda consecuencia es que, al existir tantas combinaciones de los features, pasa a haber una enorme cantidad de regiones distintas en la función que intentamos aproximar, por lo que muchos métodos no pueden capturar la forma de una función tan compleja.\n",
    "\n",
    "* Una forma de mitigarlo es mediante **_Principal Components Analysis_** (PCA), que nos ayuda a reducir la dimensionalidad de nuestro dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No free lunch theorem\n",
    "\n",
    "    \"All models are wrong, but some models are useful.\"\n",
    "\n",
    "                         — George Box (Box and Draper 1987, p424)\n",
    "                         \n",
    "*No free lunch theorem* (Wolpert, 1996) establece que no existe un modelo universalmente mejor a todos los demás, sino que cada modelo, al partir de diversas asunciones, tiene sus ventajas y desventajas. Esto puede hacer que un modelo desempeñarse muy bien en un cierto dominio y muy pobremente en otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occam's Razor\n",
    "\n",
    "    “When presented with competing hypothetical answers to a problem, one should select the one that makes the fewest assumptions”\n",
    "\n",
    "En ML puede interpretarse de varias formas, una de las cuáles establece que, ante distintos modelos igualmente competitivos para un determinado problema, debemos optar por más simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Hasta recién hemos visto dominios de datos **estructurados**, es decir dominios en donde los datos siguen una estructura definida, típicamente porque los mismos han sido recolectados de acuerdo a un modelo que sigue esa misma estructura.\n",
    "\n",
    "No obstante, muchos de los datos generados por sistemas de información no están estructurados de acuerdo a un modelo. Estos datos se conocen como datos **no estructurados** y, como tales, entrenar un modelo sobre ellos no suele ser directo, sino que, por ejemplo, se debe hacer un pre-procesamiento particular de los datos de acuerdo al dominio del problema. A continuación vamos a mostrar brevemente una de las sub-áreas de IA que trabaja con una gran parte de los datos no estructurados: Procesamiento del Lenguaje Natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de sub-área clásica de IA: Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "El procesamiento de lenguaje natural es una de las sub-áreas clásicas de la IA, junto con el procesamiento de imágenes/video, procesamiento de voz o la robótica.\n",
    "\n",
    "Como cada sub-área concreta que lleva años siendo estudiada, tiene consideraciones y técnicas particulares que suelen ser aplicables sólo en la misma.\n",
    "\n",
    "En particular, NLP apunta toma un texto como entrada y genera una salida relevante como ser el texto traducido, la identificación de la categoría a la que pertenece, análisis de sentimientos, entre otros. En nuestro ejemplo vamos a analizar el caso de la predicción de la categoría a partir de un texto.\n",
    "\n",
    "Al trabajar con datos no estructurados en forma de texto de longitud variable con sus posibles ambigüedades, errores y demás, no suele ser posible entrenar directamente un modelo de predicción sobre ellos (por ejemplo, porque los features directamente no están definidos). A continuación podemos ver una vista general de la construcción de un sistema de clasificación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/building_text_classification_system.png)\n",
    "\n",
    "Fuente: Figura 4.2 de Text Analytics with Python: A Practical Real-World Approach to Gaining Actionable Insights from Your Data (D. Sarkar, 2016). Buen libro para aquellos que quieran profundizar en NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial recomendado para iniciarse en NLP: [Working with Text Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra librería recomendadada para NLP: [Natural Language Toolkit (NLTK)](http://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curso recomendado para aquellos que desean profundizar en NLP: Procesamiento de Lenguaje Natural, dictado en la Facultad de Matemática, Astronomía y Física, UNC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones de la Introducción al Aprendizaje Supervisado\n",
    "\n",
    "* El aprendizaje supervisado permite predecir salidas de una función desconocida $f(X)$ al tomarla como una caja negra para entradas $X$ no observadas, dado un entrenamiento previo con $(X, f(X))$ conocidos.\n",
    "\n",
    "* Sus técnicas permiten obtener un gran tasa de aciertos con métodos de variada complejidad para un rango muy importante de problemas, en campos diversos como el reconocimiento de imágenes, robótica, procesamiento de texto, entre otros.\n",
    "\n",
    "* En las clases hasta aquí se mostró una introducción al aprendizaje supervisado, mostrando cuáles son sus principales características, modelos y cómo evaluarlos.\n",
    "\n",
    "* Debido a que el campo es muy amplio, muchos modelos han quedado fuera del alcance de estas clases; no obstante confiamos que al conocer las bases y al haber implementado varios, el aprendizaje de nuevas técnicas no será dificultoso puesto que la gran mayoría se como una extensión de lo visto en estas clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El aprendizaje supervisado es, dentro de machine learning, el área con más aplicaciones y avances científicos. No obstante, al igual que como sucede en todo el campo de ML, es en este momento \"más un arte que una ciencia\", debido a que las soluciones dependen mucho de la experiencia previa con la que uno cuente, sumado a la necesidad de conocer a fondo el dataset con el que se trabaja, lo que hace que no exista una metodología estándar para encarar o evaluar los problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajo Práctico 4\n",
    "\n",
    "#### Ejercicios Básicos\n",
    "\n",
    "1. Elegir dos modelos de clasificación y ajustar algún modelo de predicción en un dataset a elección, tal como venimos haciendo hasta ahora, mostrando para cada clasificador la tasa de aciertos junto con sus respectivos precision, recall y área bajo la curva. Explicar el paso a paso del análisis del dataset, su pre-procesamiento (si aplica), entrenamiento del modelo, su prueba de generalización y las mejoras realizadas para mejorar su tasa de aciertos, en el caso de haberlas realizado. Se alienta a la búsqueda de nuevos datasets y cómo implementar clasificadores, por lo que el dataset debe ser nuevo junto con al menos uno de los clasificadores seleccionados.\n",
    "\n",
    "#### Ejercicios Complementarios\n",
    "\n",
    "1. Elegir un dataset de algún concurso abierto de Kaggle, generar las predicciones con un algoritmo y participar del mismo enviando las predicciones. Referenciar el concurso y a su nombre de usuario (para su búsqueda en el leaderboard público). Comentar las decisiones particulares tomadas y el paso a paso del tuneo del algoritmo para llegar al mejor score posible (ejemplo: el uso de cierta normalización o cualquier particularidad que aplique, como el no uso de alguna técnica porque para el problema específico no está funcionando). Nota: Si bien se motiva a ver notebooks de otras personas en Kaggle, el algoritmo utilizado debe ser original (por algoritmo se hace referencia al paso a paso desde el pre-procesamiento de los datos hasta la generación final de las salidas). Se permite hacer el ejercicio básico y complementario juntos en un sólo ejercicio; si este es el caso, se permite utilizar un único dataset y no es necesario hacer el complementario con ambos modelos de clasificación y el ejercicio básico se considerará completo al emplear dos modelos de clasificación y comparar los mismos con las métricas mencionadas en dicho enunciado. \n",
    "\n",
    "#### Ejercicios Extra\n",
    "\n",
    "Para este práctico, el puntaje del ejercicio extra se incluye dentro de los ejercicios complementarios.\n",
    "\n",
    "\n",
    "\n",
    "Fecha de entrega: ** 05/06/2018 23:55**.\n",
    "\n",
    "Nota: Como medida excepcional, y sólo para aquellos que decidan hacer el Ejercicio Complementario de este práctico y envíen a Kaggle una predicción, **se permite formar equipos de hasta dos personas y entregar un práctico válido para ambos (es decir con Ejercicios Básicos y Complementarios)**. Los grupos deben ser notificados en la clase Teórica del Miércoles 16/05. En caso de no optar por hacer el ejercicio complementario, la resolución de los ejercicios sigue siendo **individual** como hasta ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas fuentes de datasets para el ejercicio básico:\n",
    "\n",
    "https://www.kaggle.com/datasets\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets.html\n",
    "\n",
    "http://scikit-learn.org/stable/datasets/index.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
