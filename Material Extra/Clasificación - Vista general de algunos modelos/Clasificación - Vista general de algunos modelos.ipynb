{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación: Vista general de algunos modelos\n",
    "\n",
    "El presente notebook se deja solamente a modo de repaso general de los contenidos ya vistos en el teórico y/o otros algoritmos. Para su estudio en profundidad, referirse a las clases teóricas en donde fueron presentados los temas y/o bibliografía de cátedra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes Classifier\n",
    "\n",
    "Retomando el Teorema de Bayes, dada una observación (una fila de $X$) que asumimos **IID** $x_1, x_2, ..., x_n$, \n",
    "\n",
    "$$P(y \\mid x_1, x_2, ..., x_n) = \\frac{P(y) P(x_1, x_2, ..., x_n \\mid y)}{P(x_1, x_2, ..., x_n)}$$\n",
    "\n",
    "... Naïve Bayes Classifier es un clasificador que intenta aproximar $P(y \\mid x_1, x_2, ..., x_n)$ tomando la muy simplista (_naïve_) asunción de que los predictores son independientes entre sí. Analíticamente,\n",
    "\n",
    "$$P(x_i \\mid y, x_1, ..., x_{i-1}, x_{i+1}, ..., x_n) = P(x_i \\mid y)$$\n",
    "\n",
    "Recordemos que dados $A$, $B$, si $A$ es independiente de $B$ entonces $P(A \\cap B) = P(A)P(B)$. Entonces la primera ecuación se puede reescribir como\n",
    "\n",
    "$$P(y \\mid x_1, x_2, ..., x_n) = \\frac{P(y) \\prod_{i=1}^n P(x_i \\mid y)}{P(x_1, x_2, ..., x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando que $P(x_1, x_2, ..., x_n) = \\sum_{c=1}^C P(y_c)P(x_1, x_2, ..., x_n \\mid y_c)$ es constante, se deduce que\n",
    "\n",
    "$$P(y \\mid x_1, x_2, ..., x_n) \\varpropto P(y) \\prod_{i=1}^n P(x_i \\mid y)$$\n",
    "\n",
    "Por lo tanto,\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_y P(y)\\prod_{i=1}^n P(x_i \\mid y)$$\n",
    "\n",
    "* Pese a la simplista asunción tomada, Naïve Bayes es uno de los mejores y más rápidos clasificadores, especialmente en lo referido a procesamiento de texto. \n",
    "* Por otra parte, no es considerado un buen estimador, por lo que las probabilidades estimadas (por medio de *predict_proba*) no deben tomarse muy seriamente. Tampoco resulta ideal para datasets con muchos features numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Una pregunta común es ¿por qué no utilizar los métodos de regresión para predecir las clases?\n",
    "\n",
    "* La dificultad es que la mayoría de los métodos de regresión trabajan asumiendo un orden sobre las salidas. Como las salidas no están ordenadas, no es posible utilizar la diferencia entre ellas para entrenar iterativamente el modelo.\n",
    "\n",
    "* Supongamos que queremos predecir la condición clínica de un paciente que llega a emergencias en base a sus síntomas. Su condición clínica puede ser {Aflicción cardíaca, Sobredosis, Reacción alérgica}. Podríamos intentar hacer una regresión de las salidas como variables cuantitativas de la siguiente forma\n",
    "\n",
    "0 - Aflicción cardíaca\n",
    "\n",
    "1 - Sobredosis\n",
    "\n",
    "2 - Reacción alérgica\n",
    "\n",
    "* Un problema es que estamos asumiendo un orden de las salidas, en donde asumimos sin fundamentos 1) que una SD está entre una AC y una RA y 2) que la diferencia entre una AC y una SD es la misma que la diferencia entre SD y RA.\n",
    "\n",
    "* Este problema podría solventarse si limitamos las clases a dos. En ese sentido, supongamos que limitamos la cantidad de clases a AC y SD; una predicción de 0.23 se asociaría con una AC, mientras que una predicción de 0.9 se asociaría con una SD. Aquí el otro problema: supongamos que usamos un método de regresión lineal, ¡el valor de predicción iría hacia el infinito, pudiendo incluir probabilidades negativas o > 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No obstante, es posible solventar esto aplicándole un \"aplanamiento\" a las salidas para que pasen de $[-\\infty,\\infty]$ a probabilidades [0,1].\n",
    "* Uno de los métodos clásicos que realizan esto recibe el nombre de regresión logística (**logistic regression**). El mismo propone modelar $P(y \\mid X)$ directamente (es un método discriminativo) a través de la función logística. Para el caso de dos clases 0 y 1:\n",
    "$$P(y = 1 \\mid X = X_0) = \\frac{\\exp(\\beta _{0} + \\beta^{T} X_0)}{1+\\exp(\\beta _{0}+\\beta ^{T} X_0)}$$\n",
    "donde $\\beta$ son los coeficientes de regresión.\n",
    "(Notar que $\\exp(x)$ equivale a $e^x$).\n",
    "\n",
    "![Logistic Regression](images/logistic_regression.png)\n",
    "\n",
    "Fuente: Figura 4.2 de Hastie et. al. 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los coeficientes $\\beta$ son desconocidos. Para estimarlos se utiliza el método de _Maximum Likelihood_. Recordemos que en estadística, la likelihood (verosimilitud) de un modelo es la probabilidad de los parámetros del mismo dados los datos. Así, la función de likelihood está dada por $\\mathbb{L}(\\theta \\mid X) = P(x_1, x_2, ..., x_n \\mid \\theta) = P(x_1 \\mid \\theta) \\times P(x_2 \\mid \\theta) \\times ... \\times P(x_n \\mid \\theta) = \\prod_{i=1}^n P(x_i \\mid \\theta)$.\n",
    "\n",
    "* El objetivo del método Maximum Likelihood es encontrar los parámetros $\\theta$ para los cuales se maximiza la verosimilitud de los datos (en otras palabras, bajo qué parámetros es más probable que los datos hayan sido generados). Este método también puede utilizarse para verificar si dadas unas muestras que por ejemplo se asumen bajo una distribución normal, cuáles son los parámetros $\\mu$ y $\\sigma$ de la misma.\n",
    "\n",
    "* Dados los datos IID $x_1, x_2, ..., x_n$, el máximo likelihood está dado por\n",
    "$$\\theta_{ML} = \\arg \\max_{\\theta \\in \\Theta} \\hat{\\mathbb{l}}(\\theta \\mid x_1, x_2,..., x_n)$$\n",
    "\n",
    "* Logistic Regression es uno de los clásicos para problemas de clasificación binaria. Para clasificaciones multiclase, se utiliza el esquema One vs Rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "* Una SVM puede ser imaginada como una superficie que define un límite entre varios puntos de datos, los cuales representan ejemplos de distintas clases.\n",
    "* El objetivo de una SVM es crear un límite, llamado hiperplano, que separe las particiones de datos de la forma más homogénea y con mayor distancia posible.\n",
    "* Es uno de los mejores métodos de clasificación, habiendo explotado en popularidad en los últimos años. Puede ser adaptado para casi cualquier problema de aprendizaje; su enorme flexibilidad hace que sea un excelente método empleado en campos como reconocimiento de patrones en una imágenes, procesamiento de texto y detección de eventos muy raros.\n",
    "\n",
    "![SVM](images/hyper-planes.png)\n",
    "\n",
    "Fuente: https://pradeepadhokshaja.blogspot.com.ar/2016/06/optical-character-recognition-using.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En un espacio $d$-dimensional, un hiperplano se define como un sub-espacio de dimensión ($d-1$). Ejemplo: en tres dimensiones, un hiperplano equivale al plano tal como fue estudiado en Geometría Analítica. Recordemos que un plano es el análogo en dos dimensiones a un punto ($d=0$) y a una línea ($d=1$).\n",
    "\n",
    "* En dos dimensiones, un hiperplano se define por la ecuación \n",
    "$$\\beta_0 + \\beta_1 x + \\beta_2 y = 0$$\n",
    "Dado un punto $(x^*,y^*)$ que no satisface la anterior ecuación sino que\n",
    "$$\\beta_0 + \\beta_1 x^* + \\beta_2 y^* > 0$$\n",
    "decimos que $(x^*,y^*)$ se encuentra hacia uno de los lados del plano.\n",
    "Análogamente, si $$\\beta_0 + \\beta_1 x^* + \\beta_2 y^* < 0$$ diremos que $(x^*,y^*)$ se encuentra hacia el otro lado del plano (si esto no queda claro a simple vista, imaginar el ejemplo de una recta. Se muestran ejemplos en la figura).\n",
    "\n",
    "![](images/hyperplanes.png)\n",
    "\n",
    "Fuente: https://pradeepadhokshaja.blogspot.com.ar/2016/06/optical-character-recognition-using.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extendiendo esta noción a un espacio $d$-dimensional, y considerando que queremos clasificar cada observación en el conjunto binario de clases $C = \\{-1,1\\}$, de forma que \n",
    "$$\\beta_0 + \\beta_1 x_1^* +...+ \\beta_d x_d^* > 0$$\n",
    "si la observación $y_i=1$, y\n",
    "$$\\beta_0 + \\beta_1 x_1^* +...+ \\beta_d x_d^* < 0$$ si la observación $y_i = -1$\n",
    "\n",
    "* Suponiendo que existe un hiperplano capaz de separar perfectamente las observaciones de acuerdo a cada clase, el mismo consituye un clasificador que naturalmente separa las clases clasificadas como -1 de las clasificadas con 1, de forma que\n",
    "$$y_i (\\beta_0 + \\beta_1 X_{i1}^* +...+ \\beta_d X_{id}^*) > 0$$\n",
    "dada una matriz X de datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esencialmente, lo que hace una SVM es encontrar aquel hiperplano que maximiza el margen que separa las clases (**maximum margin hyperplane, MMH**), puesto la misma es la que se espera minimice el error de clasificación. Aquellos puntos ubicados en tales márgenes se denominan **support vectors**.\n",
    "\n",
    "![](images/maximum_margin.png)\n",
    "\n",
    "Fuente: https://pradeepadhokshaja.blogspot.com.ar/2016/06/optical-character-recognition-using.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Un problema se origina cuando los datos con los que contamos no son linealmente separables como en el caso de la figura anterior.\n",
    "* En tal caso, dos enfoques suelen usarse. En el primero, el algoritmo utiliza emplea un margen flexible denotado por $\\xi$, además de una función de costo $C$ que es aplicada para los casos en los cuales un punto no pertenece correctamente a su hiperplano. El objetivo de SVM pasa ahora a ser la minimización de la función de costo $C$.\n",
    "\n",
    "![](images/soft_margin.png)\n",
    "\n",
    "Fuente: https://pradeepadhokshaja.blogspot.com.ar/2016/06/optical-character-recognition-using.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El segundo enfoque, por otra parte, consiste en utilizar un operador (kernel) no lineal en lugar de utilizar el producto punto, de modo de poder generar un MMH en otro espacio.\n",
    "\n",
    "![](images/kernel_trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "* Un árbol de decisión es un modelo predictivo que intenta explicar los datos $(X, y)$ como conjuntos de **reglas de decisión**. Recibe ese nombre porque puede representarse como una estructura de árbol.\n",
    "* Se compone de dos tipos de nodos: los internos y las hojas. Los nodos internos definen reglas de decisión que consultan si una determinada condición es satisfecha. Cada nodo no hoja amplía en 1 la profundidad del árbol.\n",
    "* Las hojas, por su parte, tienen un valor de predicción (para el caso de los árboles de regresión) o una clase (para los árboles de clasificación), dependiendo del problema que estén resolviendo.\n",
    "* Técnica con altos rendimientos e interpretable, pero **muy propensa al overfitting**: la profundidad elegida del árbol es muy importante; es deseable \"podar\" el árbol al reducir su profundidad para evitar que el overfitting llegue a límites muy altos.\n",
    "\n",
    "![Regla de decisión](images/decision_rule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Árbol de decisión](images/obama_tree.png)\n",
    "\n",
    "Fuente: New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "* Los métodos de Ensemble Learning combinan varios modelos para resolver un problema de predicción. Cada uno de esos modelos aprenden y realizan predicciones independientemente, para luego combinar las mismas de forma tal de generar una única predicción igual o mejor que cualquier predicción realizada por un único modelo.\n",
    "* Se conocen como modelos débiles, puesto que necesitan de modelos específicos para poder combinar sus predicciones.\n",
    "* Un método muy conocido de ensemble es **Bootstrap Aggregating** (conocido como **bagging**).\n",
    "\n",
    "![](images/bagging.png)\n",
    "\n",
    "Fuente: Udacity Course - Machine Learning for Trading (por Georgia Tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "* Random Forest es un método de ensemble learning que agrega múltiples árboles de decisión.\n",
    "* Es una mejora del bagging, que es un método que hace bootstrapping sobre varios modelos y promedia sus salidas para obtener la media (para problemas de regresión) o la clase seleccionada por votación (para problemas de clasificación).\n",
    "* Por medio de bootstrapping sobre cada una de las divisiones del árbol, los Random Forest crean automáticamente una alta cantidad de árboles de decisión sobre los mismos, con el objetivo de encontrar árboles de decisión que  obteniendo la salida tomando la media de todas las predicciones de los árboles creados para problemas de regresión, o bien eligiendo la clase de salida mediante votación para problemas de clasificación.\n",
    "* La gran mayoría de tales árboles generados automáticamente arrojan pésimas predicciones; no obstante las mismas se cancelan entre sí dando lugar a aquellos árboles que mejor se ajustan a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ventajas\n",
    "* Tiene rendimientos a nivel del estado del arte, al igual que las SVM.\n",
    "\n",
    "Desventajas\n",
    "* Es un método muy propenso al overfitting.\n",
    "* Al usar los árboles de decisión de esta manera, se pierde bastante de su interpretabilidad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
